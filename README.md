# ğŸ§  Neuron Connectivity Analyzer

## ğŸ“‹ å››é˜¶æ®µå·¥ä½œæµç¨‹

### 1. ğŸ¯ åŸå§‹SWCæ•°æ®å¤„ç†
ä»å•ç¥ç»å…ƒSWCå½¢æ€å­¦æ•°æ®ä¸­æå–æœ‰å‘è¿æ¥è·¯å¾„

**åŠŸèƒ½ç‰¹ç‚¹**ï¼š
- SWCæ ¼å¼è§£æä¸é¢„å¤„ç†
- è„‘åŒºæ ‡æ³¨æ˜ å°„
- æœ‰å‘å›¾ç½‘ç»œæ„å»º
- æ‹“æ‰‘æ’åºä¼˜åŒ–

```python
# åŠ è½½è„‘åŒºæ³¨è§£æ•°æ®
anno = pyswcloader.brain.read_nrrd('data/annotation_25.nrrd')
resolution = 25  # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
allen_brain_tree = pyswcloader.brain.allen_brain_tree('data/1.json')
stl_acro_dict = pyswcloader.brain.acronym_dict('data/1.json')

# åˆ›å»ºSWCæ‰¹é‡å¤„ç†å™¨
batch_processor = BatchSWCProcessor(anno, resolution)

# å¤„ç†å•ç¥ç»å…ƒSWCæ•°æ®
# è¯¥è·¯å¾„ä¸‹å­˜æœ‰éƒ¨åˆ†åŸå§‹swcç¤ºä¾‹æ•°æ®
# å®Œæ•´æ•°æ®é›†å·²æ•´ç†å¹¶æ”¾ç½®åœ¨ data\neuron_path_data\zip_fold ç›®å½•ä¸‹
root_path = "data/orig_swc_data/test/unzip/"

# æ‰§è¡Œæ‰¹é‡å¤„ç†
results = batch_processor.process_batch_folders(root_path)
```



### 2. ğŸ” ä»£è¡¨æ€§ç‰¹å¾è·¯å¾„æ•´åˆ
ä»å¤§é‡è¿æ¥è·¯å¾„ä¸­è¯†åˆ«å…³é”®ç‰¹å¾è·¯å¾„

**åŠŸèƒ½ç‰¹ç‚¹**ï¼š
- è·¯å¾„é¢‘ç‡ç»Ÿè®¡åˆ†æ
- è´ªå¿ƒç®—æ³•ç‰¹å¾è·¯å¾„è¯†åˆ«

```python
#åŠ è½½æ•°æ®,ä¸‹é¢å±•ç¤ºçš„ä¸ºç¤ºä¾‹æ•°æ®é›†
#directed_dfä¸ºå°é¼ è„‘ç‰©ç†é‚»æ¥çŸ©é˜µ
#all_regional_paths.csvä¸ºä¸Šä¸€æ­¥å•ç¥ç»å…ƒæ•´ç†å‡ºçš„è·¯å¾„ä¿¡æ¯
directed_df=pd.read_csv('data/Mouse_brain_adjacency_matrix.csv',index_col=0)
file = pd.read_csv('data/orig_swc_data/test/unzip/all_regional_paths.csv')

combined_df = pd.concat([file], ignore_index=True)


# åˆ›å»ºå¤„ç†å™¨
processor_swc = SWCPathProcessor(allen_brain_tree, stl_acro_dict)
# å¤„ç†è·¯å¾„æ•°æ®
keys_set = processor_swc.filter_problematic_nodes(directed_df, stl_acro_dict)

combined_df = processor_swc.process_path_pipeline(combined_df,keys_set)

# æ„å»ºä»£è¡¨æ€§è·¯å¾„
unique_pairs = processor_swc.build_representative_paths(
    combined_df, 
    save_progress_path='data/neuron_path_data/example/progress.csv'
)

#å¯¹ä»£è¡¨æ€§è·¯å¾„è¿›è¡Œæ³¨é‡Š
unique_pairs['replaced_path'] = unique_pairs['path'].apply(
    processor_swc.replace_nodes_with_acronyms
)

unique_pairs['replaced_start_node'] = unique_pairs['replaced_path'].str.split('â†’').str[0]

unique_pairs.to_csv('data/neuron_path_data/example/result.csv', index=False)


```



### 3. ğŸ“Š å®éªŒæ•°æ®èåˆä¸è®­ç»ƒé›†æ„å»º
å°†ç‰¹å¾è·¯å¾„ä¸å®éªŒæŠ•å°„å¼ºåº¦æ•°æ®ç»“åˆ

**åŠŸèƒ½ç‰¹ç‚¹**ï¼š
- Allenæ•°æ®ä¸‹è½½
- æŠ•å°„å¼ºåº¦æ ‡ç­¾å¯¹é½
- åŸºå› è¡¨è¾¾æ•°æ®æ•´åˆ
- æ•°æ®æ ‡å‡†åŒ–å¤„ç†
- è®­ç»ƒ/éªŒè¯é›†åˆ†å‰²



```python
#ä¸‹è½½æ•°æ®,è¿™é‡Œåªä¸‹è½½äº†10ä¾‹æ•°æ®å·¦å³ä½œä¸ºç¤ºèŒƒï¼Œå¦‚æœæ‚¨æƒ³è¦ä¸‹è½½æ›´å¤šæ•°æ®ï¼Œå¯ä»¥é€šè¿‡æˆ‘ä»¬æä¾›çš„æ•´ä½“å®éªŒæ•°æ®url.csv
AllenData = AllenDataFusion(allen_brain_tree, stl_acro_dict)

AllenData.download_Allen_files(  
    csv_file_path='data/experiment/url.csv',
    download_dir='data/experiment/injection_fraction',
    image_type="injection_fraction"
)

AllenData.download_Allen_files(  
    csv_file_path='data/experiment/url.csv',
    download_dir='data/experiment/projection_density',
    image_type="projection_density"
)



all_experiments = pd.read_csv('data/experiment/url.csv')
# åˆå§‹åŒ–èåˆå™¨
fusion_processor = AllenDataFusion(anno, allen_brain_tree, stl_acro_dict)

# é¢„å¤„ç†æ³¨è§£æ•°æ®ï¼ˆåªåšä¸€æ¬¡ï¼‰ï¼Œæå…¶è€—æ—¶ä»¥åŠå ç”¨å†…å­˜
annot_labeled, area_masks, valid_areas = fusion_processor.preprocess_annotation_data()

# å®éªŒIDåˆ—è¡¨
id_list = all_experiments['id'].tolist()

æ‰¹é‡å¤„ç†å®éªŒæ•°æ®ï¼ˆé¡ºåºç‰ˆæœ¬ï¼‰
results_df = fusion_processor.batch_process_experiments_sequential(
    experiment_ids=id_list,
    annot_labeled=annot_labeled,
    area_masks=area_masks,
    valid_areas=valid_areas,
    base_dir="data/experiment/example",
    output_dir="data/experiment/example/result",
    use_projection_density=True  # ä½¿ç”¨æŠ•å½±å¯†åº¦
)



######æ•°æ®æ•´åˆï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬æä¾›äº†7319544å•ç¥ç»å…ƒè·¯å¾„ç»Ÿè®¡å®Œå…¨çš„ç‰¹å¾è·¯å¾„
unique_pairs = pd.read_csv('data/neuron_path_data/zip_fold/result.csv')

#åŠ è½½å’Œé¢„å¤„ç†Allenæ•°æ®ï¼Œè¿™é‡Œæˆ‘ä»¬æä¾›äº†2992å®Œæ•´çš„å¤„ç†åæ•°æ®
allen_data = fusion_processor.load_and_preprocess_allen_data(
    'data/experiment/merged_results.csv'
)

åˆ›å»ºåŒä¾§å’Œå¯¹ä¾§çŸ©é˜µ
ipsi_matrix, contra_matrix = fusion_processor.create_ipsi_contra_matrices(allen_data)

è¿‡æ»¤èŠ‚ç‚¹ï¼ˆéœ€è¦ä¼ å…¥keys_setï¼‰
ipsi_filtered = fusion_processor.filter_matrix_nodes(ipsi_matrix, keys_set)
contra_filtered = fusion_processor.filter_matrix_nodes(contra_matrix, keys_set)

åˆ›å»ºå±‚æ¬¡åŒ–çŸ©é˜µ,è¿™ä¸€æ­¥å¯¹åº”ç”Ÿæˆipsi_matrix_result
ipsi_hierarchical = fusion_processor.create_hierarchical_matrix(ipsi_filtered)

è¿‡æ»¤å’Œå½’ä¸€åŒ–
ipsi_processed = fusion_processor.filter_and_normalize_matrix(ipsi_hierarchical, percentile=75)

#å°†å®éªŒæ•°æ®ä¸ç¥ç»å…ƒè·¯å¾„ä¿¡æ¯æ•°æ®èåˆ
final_results = fusion_processor.integrate_paths_with_intensity(
    unique_pairs,  # æ¥è‡ªå‰ä¸€æ­¥çš„ä»£è¡¨æ€§è·¯å¾„
    ipsi_processed,
    min_path_length=5
)

#åŸºå› è¡¨è¾¾æ•°æ®é€šè¿‡æ•´ç†digtal brainçš„ç©ºè½¬æ•°æ®æ”¾åœ¨äº†data/geneç›®å½•ä¸‹
#æ¨ªåæ ‡æ˜¯åŒºåŸŸï¼Œçºµåæ ‡æ˜¯åŸºå› åï¼Œå¦‚æœæ‚¨æœ‰æ›´ä¼˜è´¨æ•°æ®é›†å¯æŒ‰ç…§è¿™ä¸ªæ ¼å¼è¿›è¡Œç¼–è¾‘

```






### 4. ğŸ§  LSTMæ¨¡å‹è®­ç»ƒä¸é¢„æµ‹
æ„å»ºåºåˆ—æ¨¡å‹é¢„æµ‹è„‘åŒºè¿æ¥å¼ºåº¦

**åŠŸèƒ½ç‰¹ç‚¹**ï¼š
- è®­ç»ƒæ•°æ®é›†æ„å»º
- LSTMåºåˆ—å»ºæ¨¡
- æ¢¯åº¦é‡è¦æ€§åˆ†æ
- è¿æ¥å¼ºåº¦é¢„æµ‹

```python
# åˆå§‹åŒ–å¤„ç†å™¨
processor_SequenceDataProcessor = SequenceDataProcessor(stl_acro_dict, 'data/gene/gene_filled_result.csv')

# åŠ è½½å’Œå‡†å¤‡æ•°æ®
X, y_log, max_len, pca = processor_SequenceDataProcessor.load_and_prepare_data('data/model/final_results.csv', window_size=3)

# åˆ†å‰²æ•°æ®
node_train, node_test, strength_train, strength_test = processor_SequenceDataProcessor.split_data(X, y_log)

# å‡†å¤‡æœ€ç»ˆæ•°æ®
gene_train, gene_test, init_strength_train, init_strength_test, strength_train_shift, strength_test_shift = processor_SequenceDataProcessor.prepare_final_data(
    node_train, node_test, strength_train, strength_test, max_len
)

# æ„å»ºæ¨¡å‹
model = processor_SequenceDataProcessor.build_true_autoregressive_model_with_k(max_len=3,gene_embed_dim=64)

# è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨å†…ç½®å›è°ƒï¼‰
r2_callback = processor_SequenceDataProcessor.MultiInputR2ScoreCallback(
    validation_data=([gene_test, init_strength_test], strength_test_shift)
)

history = model.fit(
    [gene_train, init_strength_train],
    strength_train_shift,
    validation_data=([gene_test, init_strength_test], strength_test_shift),
    epochs=50,
    batch_size=32,
    callbacks=[r2_callback]
)

gene_all = np.concatenate([gene_train, gene_test], axis=0)
init_strength_all = np.concatenate([init_strength_train, init_strength_test], axis=0)
strength_shift_all = np.concatenate([strength_train_shift, strength_test_shift], axis=0)


# è®¡ç®—åŸºå› ä»¥åŠä½ç½®é‡è¦æ€§
position_imp, dim_imp = processor_SequenceDataProcessor.compute_gene_importance(
    model=model,
    dataset=(gene_all, init_strength_all),
    target_timestep=-1,
    n_samples=20000
)

# è·å–åŸå§‹åŸºå› é‡è¦æ€§
gene_importance, gene_importance_df = processor_SequenceDataProcessor.get_gene_importance_from_pca(
    dimension_importance=dim_imp
)

```



